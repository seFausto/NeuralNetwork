import java.util.ArrayList;
import java.util.Random;

public class Neuron {

	private double _outputValue;
	private int _myIndex;
	private ArrayList<Connection> _outputWeights;
	public double _gradient = 0;
	private double eta = .5; // learning
	private double alpha = 1; // momentum
	
	public Neuron(int numberOfOutputs, int myIndex) {
		Random r = new Random();
		_outputWeights = new ArrayList<Connection>();

		for (int i = 0; i < numberOfOutputs; i++) {
			Connection c = new Connection();
			c.weight = r.nextDouble();

			_outputWeights.add(c);
		}
		
		_myIndex = myIndex;
	}

	public void SetOutputValue(double outputValue) {
		_outputValue = outputValue;
	}

	public double GetOuputValue() {
		return _outputValue;
	}

	public void FeedForward(Layer previousLayer) {
		double sum = 0.0;

		for (int n = 0; n < previousLayer.Neurons.size(); n++) {
			sum += previousLayer.Neurons.get(n).GetOuputValue()
					* previousLayer.Neurons.get(n)._outputWeights.get(_myIndex).weight;
		}

		_outputValue = Activation(sum);
	}

	public double Activation(double sum) {
		// convert to sigmoid
		return Math.tanh(sum);
	}

	public void CalcOutputGradients(double targetValue) {
		double delta = targetValue - _outputValue;
		_gradient = delta * ActivationDerivative(_outputValue);
	}

	public void CalcHiddenGradients(Layer nextLayer) {
		double dow = SumDow(nextLayer);
		_gradient = dow * ActivationDerivative(_outputValue);

	}

	public void UpdateInputWeights(Layer previousLayer) {
		
		for (int i = 0; i < previousLayer.Neurons.size() -1; i++) {
			
			Neuron neuron = previousLayer.Neurons.get(i);
			
			double oldDeltaWeight = neuron._outputWeights.get(_myIndex).deltaWeight;
			double newDeltaWeight = eta * neuron.GetOuputValue() * _gradient
					+ alpha * oldDeltaWeight;
			// eta learning Rate
			// alpha momemtum

			neuron._outputWeights.get(_myIndex).deltaWeight = newDeltaWeight;
			neuron._outputWeights.get(_myIndex).weight += newDeltaWeight;

		}

	}

	private double SumDow(Layer nextLayer) {
		double sum = 0.0;

		for (int i = 0; i < nextLayer.Neurons.size() - 1; i++) {
			sum += _outputWeights.get(i).weight
					* nextLayer.Neurons.get(i)._gradient;

		}
		return sum;
	}

	private double ActivationDerivative(double sum) {
		// convert to sigmoid
		return 1.0 - sum * sum;
	}
}
